# integrated_plagiarism_processor.py
"""
Integrated Plagiarism Processor - Smart Detection + Targeted Paraphrasing
Sistem terintegrasi yang mendeteksi plagiarisme dan melakukan paraphrasing terfokus
Author: DevNoLife
Version: 1.0
"""

import os
import json
from datetime import datetime
from smart_plagiarism_checker import SmartPlagiarismChecker
from document_processor import DocumentProcessor
import docx
from docx.enum.text import WD_COLOR_INDEX

class IntegratedPlagiarismProcessor:
    def __init__(self, synonym_file='sinonim.json', gemini_api_key=None):
        print("ğŸ¯ Initializing Integrated Plagiarism Processor...")
        
        # Initialize components
        self.plagiarism_checker = SmartPlagiarismChecker()
        self.document_processor = DocumentProcessor(synonym_file, gemini_api_key)
        
        # Color coding for different risk levels
        self.risk_colors = {
            'very_high': WD_COLOR_INDEX.RED,
            'high': WD_COLOR_INDEX.PINK,
            'medium': WD_COLOR_INDEX.YELLOW,
            'low': WD_COLOR_INDEX.GREEN,
            'processed': WD_COLOR_INDEX.TURQUOISE
        }
        
        print("âœ… Integrated system ready!")
        print("ğŸ” Plagiarism detection + ğŸ”„ Targeted paraphrasing")
    
    def analyze_and_process_document(self, file_path, aggressive_mode_for_high_risk=True, create_backup=True):
        """Analyze document for plagiarism and process high-risk paragraphs"""
        print("=" * 80)
        print("ğŸ¯ INTEGRATED PLAGIARISM ANALYSIS + PROCESSING")
        print("=" * 80)
        
        if not os.path.exists(file_path):
            print(f"âŒ File not found: {file_path}")
            return None
        
        filename = os.path.basename(file_path)
        print(f"ğŸ“„ Processing: {filename}")
        
        # Step 1: Analyze for plagiarism
        print(f"\nğŸ” STEP 1: PLAGIARISM DETECTION")
        print("-" * 50)
        
        plagiarism_results = self.plagiarism_checker.scan_document(file_path)
        
        if not plagiarism_results:
            print("âŒ Plagiarism analysis failed")
            return None
        
        # Step 2: Identify high-risk paragraphs
        summary = plagiarism_results['summary']
        high_risk_paragraphs = (\n            plagiarism_results['very_high_risk'] + \n            plagiarism_results['high_risk'] + \n            plagiarism_results['medium_risk']\n        )\n        \n        print(f\"\\nğŸ“Š PLAGIARISM ANALYSIS RESULTS:\")\n        print(f\"   â€¢ Total analyzed: {summary['total_analyzed']} paragraphs\")\n        print(f\"   â€¢ High risk: {len(plagiarism_results['very_high_risk'])} paragraphs\")\n        print(f\"   â€¢ Medium risk: {len(plagiarism_results['high_risk'])} paragraphs\")\n        print(f\"   â€¢ Low risk: {len(plagiarism_results['medium_risk'])} paragraphs\")\n        print(f\"   â€¢ Paragraphs needing attention: {len(high_risk_paragraphs)}\")\n        \n        if len(high_risk_paragraphs) == 0:\n            print(f\"\\nâœ… RESULT: Document appears to be original!\")\n            print(f\"ğŸ’¡ No paraphrasing needed based on pattern analysis\")\n            return {\n                'status': 'clean',\n                'plagiarism_results': plagiarism_results,\n                'processing_results': None\n            }\n        \n        # Step 3: Create backup\n        if create_backup:\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            backup_path = f\"{os.path.splitext(file_path)[0]}_backup_integrated_{timestamp}.docx\"\n            import shutil\n            shutil.copy2(file_path, backup_path)\n            print(f\"\\nğŸ’¾ Backup created: {backup_path}\")\n        \n        # Step 4: Targeted paraphrasing\n        print(f\"\\nğŸ”„ STEP 2: TARGETED PARAPHRASING\")\n        print(\"-\" * 50)\n        \n        processing_results = self._process_high_risk_paragraphs(\n            file_path, \n            high_risk_paragraphs, \n            aggressive_mode_for_high_risk\n        )\n        \n        # Step 5: Generate comprehensive report\n        final_results = {\n            'status': 'processed',\n            'filename': filename,\n            'timestamp': datetime.now().isoformat(),\n            'plagiarism_results': plagiarism_results,\n            'processing_results': processing_results,\n            'summary': self._generate_integrated_summary(plagiarism_results, processing_results)\n        }\n        \n        self._save_integrated_report(final_results)\n        self._print_final_summary(final_results)\n        \n        return final_results\n    \n    def _process_high_risk_paragraphs(self, file_path, high_risk_paragraphs, use_aggressive_mode):\n        \"\"\"Process only high-risk paragraphs with targeted paraphrasing\"\"\"\n        \n        if not high_risk_paragraphs:\n            return {\n                'paragraphs_processed': 0,\n                'total_changes': 0,\n                'method_used': 'none'\n            }\n        \n        print(f\"ğŸ¯ Processing {len(high_risk_paragraphs)} high-risk paragraphs...\")\n        \n        # Load document\n        doc = docx.Document(file_path)\n        \n        # Create index mapping for efficient lookup\n        high_risk_indices = {p['paragraph_index'] for p in high_risk_paragraphs}\n        \n        # Processing statistics\n        processing_stats = {\n            'paragraphs_processed': 0,\n            'total_changes': 0,\n            'very_high_processed': 0,\n            'high_processed': 0,\n            'medium_processed': 0,\n            'method_used': 'aggressive' if use_aggressive_mode else 'smart'\n        }\n        \n        # Switch to appropriate mode\n        if use_aggressive_mode:\n            self.document_processor.paraphraser.switch_mode('aggressive')\n            print(f\"ğŸš€ Using AGGRESSIVE mode for high-risk content\")\n        else:\n            self.document_processor.paraphraser.switch_mode('smart')\n            print(f\"ğŸ§  Using SMART mode for high-risk content\")\n        \n        # Process each paragraph\n        for i, paragraph in enumerate(doc.paragraphs):\n            para_index = i + 1\n            \n            # Skip if not in high-risk list\n            if para_index not in high_risk_indices:\n                continue\n            \n            para_text = paragraph.text.strip()\n            \n            if not para_text or len(para_text.split()) < 15:\n                continue\n            \n            # Find risk level for this paragraph\n            risk_level = self._get_paragraph_risk_level(para_index, high_risk_paragraphs)\n            risk_score = self._get_paragraph_risk_score(para_index, high_risk_paragraphs)\n            \n            print(f\"  ğŸ”„ Para {para_index}: {risk_level.upper()} RISK ({risk_score}%) - Processing...\")\n            \n            # Process with paraphraser\n            aggressiveness = self._get_aggressiveness_for_risk(risk_level)\n            \n            result = self.document_processor.paraphraser.process_paragraph_ultimate(\n                para_text,\n                paragraph_index=i,\n                total_paragraphs=len(doc.paragraphs),\n                aggressiveness=aggressiveness\n            )\n            \n            # Apply changes if improvement is good\n            improvement_threshold = self._get_threshold_for_risk(risk_level)\n            \n            if result['plagiarism_reduction'] >= improvement_threshold:\n                # Replace paragraph content\n                paragraph.clear()\n                paragraph.add_run(result['paraphrase'])\n                \n                # Apply color coding based on original risk level\n                color = self._get_color_for_risk(risk_level)\n                for run in paragraph.runs:\n                    run.font.highlight_color = color\n                \n                processing_stats['paragraphs_processed'] += 1\n                processing_stats['total_changes'] += result.get('changes_made', 1)\n                \n                # Update risk-specific counters\n                if risk_level == 'very_high':\n                    processing_stats['very_high_processed'] += 1\n                elif risk_level == 'high':\n                    processing_stats['high_processed'] += 1\n                elif risk_level == 'medium':\n                    processing_stats['medium_processed'] += 1\n                \n                print(f\"    âœ… SUCCESS: {result['plagiarism_reduction']:.1f}% reduction ({result['method']})\")\n            else:\n                print(f\"    âš ï¸ LIMITED: {result['plagiarism_reduction']:.1f}% reduction - Needs manual review\")\n        \n        # Save document\n        doc.save(file_path)\n        \n        print(f\"\\nğŸ“Š PROCESSING COMPLETED:\")\n        print(f\"   â€¢ Paragraphs processed: {processing_stats['paragraphs_processed']}/{len(high_risk_paragraphs)}\")\n        print(f\"   â€¢ Total changes made: {processing_stats['total_changes']}\")\n        print(f\"   â€¢ Method used: {processing_stats['method_used'].upper()}\")\n        \n        return processing_stats\n    \n    def _get_paragraph_risk_level(self, para_index, high_risk_paragraphs):\n        \"\"\"Get risk level for specific paragraph\"\"\"\n        for para in high_risk_paragraphs:\n            if para['paragraph_index'] == para_index:\n                if para['risk_score'] >= 75:\n                    return 'very_high'\n                elif para['risk_score'] >= 50:\n                    return 'high'\n                else:\n                    return 'medium'\n        return 'low'\n    \n    def _get_paragraph_risk_score(self, para_index, high_risk_paragraphs):\n        \"\"\"Get risk score for specific paragraph\"\"\"\n        for para in high_risk_paragraphs:\n            if para['paragraph_index'] == para_index:\n                return para['risk_score']\n        return 0\n    \n    def _get_aggressiveness_for_risk(self, risk_level):\n        \"\"\"Get appropriate aggressiveness level for risk\"\"\"\n        aggressiveness_map = {\n            'very_high': 0.9,  # Maximum aggressiveness\n            'high': 0.7,       # High aggressiveness  \n            'medium': 0.5,     # Medium aggressiveness\n            'low': 0.3         # Low aggressiveness\n        }\n        return aggressiveness_map.get(risk_level, 0.5)\n    \n    def _get_threshold_for_risk(self, risk_level):\n        \"\"\"Get improvement threshold for risk level\"\"\"\n        threshold_map = {\n            'very_high': 30,   # Require 30% improvement\n            'high': 25,        # Require 25% improvement\n            'medium': 20,      # Require 20% improvement  \n            'low': 15          # Require 15% improvement\n        }\n        return threshold_map.get(risk_level, 20)\n    \n    def _get_color_for_risk(self, risk_level):\n        \"\"\"Get highlighting color for risk level\"\"\"\n        color_map = {\n            'very_high': WD_COLOR_INDEX.RED,       # Red for very high risk\n            'high': WD_COLOR_INDEX.PINK,           # Pink for high risk\n            'medium': WD_COLOR_INDEX.YELLOW,       # Yellow for medium risk\n            'low': WD_COLOR_INDEX.GREEN            # Green for low risk\n        }\n        return color_map.get(risk_level, WD_COLOR_INDEX.YELLOW)\n    \n    def _generate_integrated_summary(self, plagiarism_results, processing_results):\n        \"\"\"Generate comprehensive summary\"\"\"\n        plag_summary = plagiarism_results['summary']\n        \n        if not processing_results:\n            return {\n                'overall_status': 'clean',\n                'action_taken': 'none',\n                'recommendation': 'Document appears original - no changes needed'\n            }\n        \n        processed_count = processing_results['paragraphs_processed']\n        total_high_risk = plag_summary['needs_paraphrasing']\n        \n        success_rate = (processed_count / total_high_risk * 100) if total_high_risk > 0 else 0\n        \n        if success_rate >= 80:\n            status = 'excellent'\n            recommendation = 'Most high-risk content successfully processed'\n        elif success_rate >= 60:\n            status = 'good'\n            recommendation = 'Good processing results - minor manual review recommended'\n        elif success_rate >= 40:\n            status = 'fair'\n            recommendation = 'Partial success - manual review required for remaining paragraphs'\n        else:\n            status = 'limited'\n            recommendation = 'Limited success - consider manual rewriting for unprocessed paragraphs'\n        \n        return {\n            'overall_status': status,\n            'success_rate': round(success_rate, 1),\n            'processed_paragraphs': processed_count,\n            'total_high_risk': total_high_risk,\n            'action_taken': processing_results['method_used'],\n            'recommendation': recommendation\n        }\n    \n    def _save_integrated_report(self, results):\n        \"\"\"Save comprehensive integrated report\"\"\"\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        report_file = f\"integrated_processing_report_{timestamp}.json\"\n        \n        try:\n            with open(report_file, 'w', encoding='utf-8') as f:\n                json.dump(results, f, ensure_ascii=False, indent=2, default=str)\n            \n            print(f\"ğŸ“‹ Integrated report saved: {report_file}\")\n            \n        except Exception as e:\n            print(f\"âš ï¸ Could not save report: {e}\")\n    \n    def _print_final_summary(self, results):\n        \"\"\"Print final comprehensive summary\"\"\"\n        summary = results['summary']\n        \n        print(f\"\\n\" + \"=\" * 80)\n        print(\"ğŸ‰ INTEGRATED PROCESSING SUMMARY\")\n        print(\"=\" * 80)\n        \n        print(f\"ğŸ“„ Document: {results['filename']}\")\n        print(f\"ğŸ” Analysis method: Pattern-based detection\")\n        print(f\"ğŸ”„ Processing method: Targeted paraphrasing\")\n        \n        if summary['overall_status'] == 'clean':\n            print(f\"\\nâœ… RESULT: DOCUMENT IS CLEAN\")\n            print(f\"ğŸ’¡ No plagiarism patterns detected - document appears original\")\n        else:\n            print(f\"\\nğŸ“Š PROCESSING RESULTS:\")\n            print(f\"   â€¢ Overall status: {summary['overall_status'].upper()}\")\n            print(f\"   â€¢ Success rate: {summary['success_rate']}%\")\n            print(f\"   â€¢ Processed: {summary['processed_paragraphs']}/{summary['total_high_risk']} high-risk paragraphs\")\n            print(f\"   â€¢ Method used: {summary['action_taken'].upper()}\")\n            \n            print(f\"\\nğŸ’¡ RECOMMENDATION: {summary['recommendation']}\")\n        \n        print(f\"\\nğŸ¨ COLOR CODING IN DOCUMENT:\")\n        print(f\"   ğŸ”´ Red highlight = Very high risk (processed)\")\n        print(f\"   ğŸŸ£ Pink highlight = High risk (processed)\")\n        print(f\"   ğŸŸ¡ Yellow highlight = Medium risk (processed)\")\n        print(f\"   ğŸ”’ No highlight = Protected content (unchanged)\")\n        \n        print(\"=\" * 80)\n\n\ndef main():\n    \"\"\"Main function untuk integrated processing\"\"\"\n    print(\"ğŸ¯ INTEGRATED PLAGIARISM PROCESSOR\")\n    print(\"ğŸ” Smart Detection + ğŸ”„ Targeted Paraphrasing\")\n    print(\"=\" * 80)\n    \n    # Initialize system\n    processor = IntegratedPlagiarismProcessor(\n        synonym_file='sinonim.json'\n    )\n    \n    # Process document\n    document_path = 'documents/SKRIPSI  FAHRISAL FADLI-2.docx'\n    \n    if os.path.exists(document_path):\n        print(f\"ğŸ¯ Starting integrated analysis and processing...\")\n        \n        results = processor.analyze_and_process_document(\n            file_path=document_path,\n            aggressive_mode_for_high_risk=True,  # Use aggressive mode for high-risk content\n            create_backup=True\n        )\n        \n        if results:\n            print(f\"\\nğŸ‰ Integrated processing completed successfully!\")\n            print(f\"ğŸ“‹ Check the detailed report for complete analysis\")\n            \n            if results['status'] == 'processed':\n                print(f\"\\nğŸ’¡ NEXT STEPS:\")\n                print(f\"   1. Review processed paragraphs in the document\")\n                print(f\"   2. Check color-coded highlights for different risk levels\")\n                print(f\"   3. Manually review any remaining high-risk content\")\n                print(f\"   4. Run plagiarism check again to verify improvement\")\n        else:\n            print(\"âŒ Processing failed\")\n    else:\n        print(f\"âŒ Document not found: {document_path}\")\n        print(f\"ğŸ’¡ Place your document in the 'documents/' folder\")\n\n\nif __name__ == \"__main__\":\n    main()